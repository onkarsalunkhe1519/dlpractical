import numpy as np 
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Dense 
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import StandardScaler 
from sklearn.datasets import fetch_california_housing 

# Load and prepare the dataset (California Housing dataset for demonstration) 
data = fetch_california_housing() 
X = data.data  # Features 
y = data.target  # Target values (continuous) 

# Split the dataset into training and testing sets 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) 

# Standardize the feature data 
scaler = StandardScaler() 
X_train = scaler.fit_transform(X_train) 
X_test = scaler.transform(X_test) 

# Build the Deep Feedforward Network 
model = Sequential([ 
    Dense(64, input_shape=(X_train.shape[1],), activation='relu'),  # Input layer + first hidden layer 
    Dense(32, activation='relu'),  # Second hidden layer 
    Dense(1, activation='linear')  # Output layer (1 neuron for regression) 
])

# Compile the model 
model.compile(optimizer='adam', 
              loss='mean_squared_error',  # Loss function for regression 
              metrics=['mean_absolute_error'])  # Evaluation metric 

# Train the model 
history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test)) 

# Evaluate the model on test data 
loss, mae = model.evaluate(X_test, y_test) 
print(f"Test Loss (MSE): {loss:.4f}, Test MAE: {mae:.4f}") 

# Predict values for new data 
y_pred = model.predict(X_test) 

# Print sample predictions 
print(f"True Values: {y_test[:5]}") 
print(f"Predicted Values: {y_pred[:5].flatten()}")
